@incollection{Wongsuphasawat2018,
author = {Wongsuphasawat, Kanit and Smilkov, Daniel and Wexler, James and Wilson, Jimbo and Mane, Dandelion and Fritz, Doug and Krishnan, Dilip and Viegas, Fernanda B. and Wattenberg, Martin},
doi = {10.1109/TVCG.2017.2744878},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/2018-TensorFlowGraph-VAST.pdf:pdf;:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/04308636.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
month = {jan},
number = {1},
pages = {1--12},
title = {{Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow}},
url = {http://ieeexplore.ieee.org/document/8019861/},
volume = {24},
year = {2018}
}
@incollection{Sugiyama1981,
author = {Sugiyama, Kozo and Tagawa, Shojiro and Toda, Mitsuhiko},
doi = {10.1109/TSMC.1981.4308636},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/04308636.pdf:pdf},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
number = {2},
pages = {109--125},
title = {{Methods for Visual Understanding of Hierarchical System Structures}},
url = {http://ieeexplore.ieee.org/document/4308636/},
volume = {11},
year = {1981}
}
@incollection{Smilkov2017,
abstract = {The recent successes of deep learning have led to a wave of interest from non-experts. Gaining an understanding of this technology, however, is difficult. While the theory is important, it is also helpful for novices to develop an intuitive feel for the effect of different hyperparameters and structural variations. We describe TensorFlow Playground, an interactive, open sourced visualization that allows users to experiment via direct manipulation rather than coding, enabling them to quickly build an intuition about neural nets.},
archivePrefix = {arXiv},
arxivId = {1708.03788},
author = {Smilkov, Daniel and Carter, Shan and Sculley, D. and Vi{\'{e}}gas, Fernanda B. and Wattenberg, Martin},
eprint = {1708.03788},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/1708.03788-2.pdf:pdf},
month = {aug},
title = {{Direct-Manipulation Visualization of Deep Networks}},
url = {http://arxiv.org/abs/1708.03788},
year = {2017}
}
@article{Yosinski2015,
  title={Understanding neural networks through deep visualization},
  author={Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  journal={arXiv preprint arXiv:1506.06579},
  year={2015}
}
@incollection{Samek2017,
author = {Samek, Wojciech and Binder, Alexander and Montavon, Gregoire and Lapuschkin, Sebastian and Muller, Klaus-Robert},
doi = {10.1109/TNNLS.2016.2599820},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/1509.06321.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
month = {nov},
number = {11},
pages = {2660--2673},
title = {{Evaluating the Visualization of What a Deep Neural Network Has Learned}},
url = {http://ieeexplore.ieee.org/document/7552539/},
volume = {28},
year = {2017}
}
@article{Nguyen2016,
abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
archivePrefix = {arXiv},
arxivId = {1602.03616},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
eprint = {1602.03616},
file = {::},
month = {feb},
title = {{Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks}},
url = {http://arxiv.org/abs/1602.03616},
year = {2016}
}
@article{Li2017,
  author    = {Hao Li and
               Zheng Xu and
               Gavin Taylor and
               Tom Goldstein},
  title     = {Visualizing the Loss Landscape of Neural Nets},
  journal   = {CoRR},
  volume    = {abs/1712.09913},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.09913},
  archivePrefix = {arXiv},
  eprint    = {1712.09913},
  timestamp = {Mon, 13 Aug 2018 16:48:49 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-09913},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{He2015,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{Liu2018, 
author={M. {Liu} and J. {Shi} and K. {Cao} and J. {Zhu} and S. {Liu}}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Analyzing the Training Processes of Deep Generative Models}, 
year={2018}, 
volume={24}, 
number={1}, 
pages={77-87}, 
keywords={data analysis;data visualisation;learning (artificial intelligence);time series;machine learning;credit assignment algorithm;visual clutter reduction;blue-noise polyline sampling scheme;visual analytics approach;training DGM;training dynamics;deep generative models;training failure;failed training process;time series samples;Training;Neurons;Time series analysis;Tools;Visual analytics;Analytical models;deep learning;deep generative models;blue noise sampling;credit assignment}, 
doi={10.1109/TVCG.2017.2744938}, 
ISSN={1077-2626}, 
month={Jan},}
@article{Hohman2018,
  title={Visual analytics in deep learning: An interrogative survey for the next frontiers},
  author={Hohman, Fred Matthew and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
  journal={IEEE transactions on visualization and computer graphics},
  year={2018},
  publisher={IEEE}
}
@article{Liu2016,
  title={Towards better analysis of deep convolutional neural networks},
  author={Liu, Mengchen and Shi, Jiaxin and Li, Zhen and Li, Chongxuan and Zhu, Jun and Liu, Shixia},
  journal={IEEE transactions on visualization and computer graphics},
  volume={23},
  number={1},
  pages={91--100},
  year={2016},
  publisher={IEEE}
}
@techreport{KarrasNVIDIA,
abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties , and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
archivePrefix = {arXiv},
arxivId = {1812.04948v3},
author = {Karras and Laine, Aila},
eprint = {1812.04948v3},
file = {::},
year={2019},
mendeley-groups = {Seminar in Computer Graphics},
title = {{A Style-Based Generator Architecture for Generative Adversarial Networks Timo Aila NVIDIA}},
url = {https://github.com/NVlabs/stylegan}
}
@techreport{Wang,
abstract = {Generative models bear promising implications to learn data representations in an unsupervised fashion with deep learning. Generative Adversarial Nets (GAN) is one of the most popular frameworks in this arena. Despite the promising results from different types of GANs, in-depth understanding on the adversarial training process of the models remains a challenge to domain experts. The complexity and the potential long-time training process of the models make it hard to evaluate, interpret, and optimize them. In this work, guided by practical needs from domain experts, we design and develop a visual analytics system, GANViz, aiming to help experts understand the adversarial process of GANs in-depth. Specifically, GANViz evaluates the model performance of two subnetworks of GANs, provides evidence and interpretations of the models' performance, and empowers comparative analysis with the evidence. Through our case studies with two real-world datasets, we demonstrate that GANViz can provide useful insight into helping domain experts understand, interpret, evaluate, and potentially improve GAN models.},
author = {Wang, Junpeng and Gou, Liang and Yang, Hao and Shen, Han-Wei},
file = {::},
year={2015},
keywords = {Index Terms-Generative adversarial nets,deep learning,model interpretation,visual analytics},
mendeley-groups = {Seminar in Computer Graphics},
title = {{GANViz: A Visual Analytics Approach to Understand the Adversarial Game}},
url = {https://junpengw.bitbucket.io/image/research/pvis18.pdf}
}
@article{Erhan2009,
  title={Visualizing higher-layer features of a deep network},
  author={Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year={2009}
}
@incollection{LeCun1998,
author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and M{\"{u}}ller, Klaus -Robert},
doi = {10.1007/3-540-49430-8_2},
mendeley-groups = {Seminar in Computer Graphics},
pages = {9--50},
publisher = {Springer, Berlin, Heidelberg},
title = {{Efficient BackProp}},
url = {http://link.springer.com/10.1007/3-540-49430-8{\_}2},
year = {1998}
}
@inproceedings{Deng2009,
author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and {Kai Li} and {Li Fei-Fei}},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206848},
isbn = {978-1-4244-3992-8},
mendeley-groups = {Seminar in Computer Graphics},
month = {jun},
pages = {248--255},
publisher = {IEEE},
title = {{ImageNet: A large-scale hierarchical image database}},
url = {https://ieeexplore.ieee.org/document/5206848/},
year = {2009}
}
@techreport{Karpathy,
abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3{\%} to 63.9{\%}), but only a surprisingly modest improvement compared to single-frame models (59.3{\%} to 60.9{\%}). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3{\%} up from 43.9{\%}).},
author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
file = {::},
mendeley-groups = {Seminar in Computer Graphics},
title = {{Large-scale Video Classification with Convolutional Neural Networks}},
url = {http://cs.stanford.edu/people/karpathy/deepvideo}
}
@article{Abadi2016,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
file = {::},
mendeley-groups = {Seminar in Computer Graphics},
month = {mar},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
year = {2016}
}
@article{Jia2014,
abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU ({\$}\backslashapprox{\$} 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
archivePrefix = {arXiv},
arxivId = {1408.5093},
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
eprint = {1408.5093},
file = {::},
mendeley-groups = {Seminar in Computer Graphics},
month = {jun},
title = {{Caffe: Convolutional Architecture for Fast Feature Embedding}},
url = {http://arxiv.org/abs/1408.5093},
year = {2014}
}
@techreport{Cho,
abstract = {In this paper, we propose a novel neu-ral network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and de-coder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
author = {Cho, Kyunghyun and {Van Merri{\"{e}}nboer}, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
file = {::},
mendeley-groups = {Seminar in Computer Graphics},
pages = {1724--1734},
year = {2014},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {https://www.aclweb.org/anthology/D14-1179}
}
@article{Choo2018,
abstract = {Recently, deep learning has been advancing the state of the art in artificial intelligence to a new level, and humans rely on artificial intelligence techniques more than ever. However, even with such unprecedented advancements, the lack of explanation regarding the decisions made by deep learning models and absence of control over their internal processes act as major drawbacks in critical decision-making processes, such as precision medicine and law enforcement. In response, efforts are being made to make deep learning interpretable and controllable by humans. In this paper, we review visual analytics, information visualization, and machine learning perspectives relevant to this aim, and discuss potential challenges and future research directions.},
archivePrefix = {arXiv},
arxivId = {1804.02527},
author = {Choo, Jaegul and Liu, Shixia},
eprint = {1804.02527},
file = {::},
mendeley-groups = {Seminar in Computer Graphics},
month = {apr},
title = {{Visual Analytics for Explainable Deep Learning}},
url = {http://arxiv.org/abs/1804.02527},
year = {2018}
}
@article{Shwartz-Ziv2017,
abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the $\backslash$textit{\{}Information Plane{\}}; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on {\{}$\backslash$emph compression{\}} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
archivePrefix = {arXiv},
arxivId = {1703.00810},
author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
eprint = {1703.00810},
file = {::},
mendeley-groups = {Seminar in Computer Graphics},
month = {mar},
title = {{Opening the Black Box of Deep Neural Networks via Information}},
url = {http://arxiv.org/abs/1703.00810},
year = {2017}
}
