@incollection{Wongsuphasawat2018,
author = {Wongsuphasawat, Kanit and Smilkov, Daniel and Wexler, James and Wilson, Jimbo and Mane, Dandelion and Fritz, Doug and Krishnan, Dilip and Viegas, Fernanda B. and Wattenberg, Martin},
doi = {10.1109/TVCG.2017.2744878},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/2018-TensorFlowGraph-VAST.pdf:pdf;:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/04308636.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
month = {jan},
number = {1},
pages = {1--12},
title = {{Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow}},
url = {http://ieeexplore.ieee.org/document/8019861/},
volume = {24},
year = {2018}
}
@incollection{Sugiyama1981,
author = {Sugiyama, Kozo and Tagawa, Shojiro and Toda, Mitsuhiko},
doi = {10.1109/TSMC.1981.4308636},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/04308636.pdf:pdf},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
number = {2},
pages = {109--125},
title = {{Methods for Visual Understanding of Hierarchical System Structures}},
url = {http://ieeexplore.ieee.org/document/4308636/},
volume = {11},
year = {1981}
}
@incollection{Smilkov2017,
abstract = {The recent successes of deep learning have led to a wave of interest from non-experts. Gaining an understanding of this technology, however, is difficult. While the theory is important, it is also helpful for novices to develop an intuitive feel for the effect of different hyperparameters and structural variations. We describe TensorFlow Playground, an interactive, open sourced visualization that allows users to experiment via direct manipulation rather than coding, enabling them to quickly build an intuition about neural nets.},
archivePrefix = {arXiv},
arxivId = {1708.03788},
author = {Smilkov, Daniel and Carter, Shan and Sculley, D. and Vi{\'{e}}gas, Fernanda B. and Wattenberg, Martin},
eprint = {1708.03788},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/1708.03788-2.pdf:pdf},
month = {aug},
title = {{Direct-Manipulation Visualization of Deep Networks}},
url = {http://arxiv.org/abs/1708.03788},
year = {2017}
}
@article{Yosinski2015,
  title={Understanding neural networks through deep visualization},
  author={Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  journal={arXiv preprint arXiv:1506.06579},
  year={2015}
}
@incollection{Samek2017,
author = {Samek, Wojciech and Binder, Alexander and Montavon, Gregoire and Lapuschkin, Sebastian and Muller, Klaus-Robert},
doi = {10.1109/TNNLS.2016.2599820},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/1509.06321.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
month = {nov},
number = {11},
pages = {2660--2673},
title = {{Evaluating the Visualization of What a Deep Neural Network Has Learned}},
url = {http://ieeexplore.ieee.org/document/7552539/},
volume = {28},
year = {2017}
}
@article{Nguyen2016,
abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
archivePrefix = {arXiv},
arxivId = {1602.03616},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
eprint = {1602.03616},
file = {::},
month = {feb},
title = {{Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks}},
url = {http://arxiv.org/abs/1602.03616},
year = {2016}
}
@article{Li2017,
  author    = {Hao Li and
               Zheng Xu and
               Gavin Taylor and
               Tom Goldstein},
  title     = {Visualizing the Loss Landscape of Neural Nets},
  journal   = {CoRR},
  volume    = {abs/1712.09913},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.09913},
  archivePrefix = {arXiv},
  eprint    = {1712.09913},
  timestamp = {Mon, 13 Aug 2018 16:48:49 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-09913},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{He2015,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{Liu2018, 
author={M. {Liu} and J. {Shi} and K. {Cao} and J. {Zhu} and S. {Liu}}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Analyzing the Training Processes of Deep Generative Models}, 
year={2018}, 
volume={24}, 
number={1}, 
pages={77-87}, 
keywords={data analysis;data visualisation;learning (artificial intelligence);time series;machine learning;credit assignment algorithm;visual clutter reduction;blue-noise polyline sampling scheme;visual analytics approach;training DGM;training dynamics;deep generative models;training failure;failed training process;time series samples;Training;Neurons;Time series analysis;Tools;Visual analytics;Analytical models;deep learning;deep generative models;blue noise sampling;credit assignment}, 
doi={10.1109/TVCG.2017.2744938}, 
ISSN={1077-2626}, 
month={Jan},}
@article{Hohman2018,
  title={Visual analytics in deep learning: An interrogative survey for the next frontiers},
  author={Hohman, Fred Matthew and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
  journal={IEEE transactions on visualization and computer graphics},
  year={2018},
  publisher={IEEE}
}
@article{Liu2016,
  title={Towards better analysis of deep convolutional neural networks},
  author={Liu, Mengchen and Shi, Jiaxin and Li, Zhen and Li, Chongxuan and Zhu, Jun and Liu, Shixia},
  journal={IEEE transactions on visualization and computer graphics},
  volume={23},
  number={1},
  pages={91--100},
  year={2016},
  publisher={IEEE}
}
@techreport{KarrasNVIDIA,
abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties , and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
archivePrefix = {arXiv},
arxivId = {1812.04948v3},
author = {Karras and Laine, Aila},
eprint = {1812.04948v3},
file = {::},
year={2019},
mendeley-groups = {Seminar in Computer Graphics},
title = {{A Style-Based Generator Architecture for Generative Adversarial Networks Timo Aila NVIDIA}},
url = {https://github.com/NVlabs/stylegan}
}
@techreport{Wang,
abstract = {Generative models bear promising implications to learn data representations in an unsupervised fashion with deep learning. Generative Adversarial Nets (GAN) is one of the most popular frameworks in this arena. Despite the promising results from different types of GANs, in-depth understanding on the adversarial training process of the models remains a challenge to domain experts. The complexity and the potential long-time training process of the models make it hard to evaluate, interpret, and optimize them. In this work, guided by practical needs from domain experts, we design and develop a visual analytics system, GANViz, aiming to help experts understand the adversarial process of GANs in-depth. Specifically, GANViz evaluates the model performance of two subnetworks of GANs, provides evidence and interpretations of the models' performance, and empowers comparative analysis with the evidence. Through our case studies with two real-world datasets, we demonstrate that GANViz can provide useful insight into helping domain experts understand, interpret, evaluate, and potentially improve GAN models.},
author = {Wang, Junpeng and Gou, Liang and Yang, Hao and Shen, Han-Wei},
file = {::},
year={2015},
keywords = {Index Terms-Generative adversarial nets,deep learning,model interpretation,visual analytics},
mendeley-groups = {Seminar in Computer Graphics},
title = {{GANViz: A Visual Analytics Approach to Understand the Adversarial Game}},
url = {https://junpengw.bitbucket.io/image/research/pvis18.pdf}
}
