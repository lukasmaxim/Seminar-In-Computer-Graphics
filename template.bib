@incollection{Wongsuphasawat2018,
author = {Wongsuphasawat, Kanit and Smilkov, Daniel and Wexler, James and Wilson, Jimbo and Mane, Dandelion and Fritz, Doug and Krishnan, Dilip and Viegas, Fernanda B. and Wattenberg, Martin},
doi = {10.1109/TVCG.2017.2744878},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/2018-TensorFlowGraph-VAST.pdf:pdf;:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/04308636.pdf:pdf},
issn = {1077-2626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
month = {jan},
number = {1},
pages = {1--12},
title = {{Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow}},
url = {http://ieeexplore.ieee.org/document/8019861/},
volume = {24},
year = {2018}
}
@incollection{Sugiyama1981,
author = {Sugiyama, Kozo and Tagawa, Shojiro and Toda, Mitsuhiko},
doi = {10.1109/TSMC.1981.4308636},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/04308636.pdf:pdf},
issn = {0018-9472},
journal = {IEEE Transactions on Systems, Man, and Cybernetics},
number = {2},
pages = {109--125},
title = {{Methods for Visual Understanding of Hierarchical System Structures}},
url = {http://ieeexplore.ieee.org/document/4308636/},
volume = {11},
year = {1981}
}
@incollection{Smilkov2017,
abstract = {The recent successes of deep learning have led to a wave of interest from non-experts. Gaining an understanding of this technology, however, is difficult. While the theory is important, it is also helpful for novices to develop an intuitive feel for the effect of different hyperparameters and structural variations. We describe TensorFlow Playground, an interactive, open sourced visualization that allows users to experiment via direct manipulation rather than coding, enabling them to quickly build an intuition about neural nets.},
archivePrefix = {arXiv},
arxivId = {1708.03788},
author = {Smilkov, Daniel and Carter, Shan and Sculley, D. and Vi{\'{e}}gas, Fernanda B. and Wattenberg, Martin},
eprint = {1708.03788},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/1708.03788-2.pdf:pdf},
month = {aug},
title = {{Direct-Manipulation Visualization of Deep Networks}},
url = {http://arxiv.org/abs/1708.03788},
year = {2017}
}
@article{Yosinski2015,
  title={Understanding neural networks through deep visualization},
  author={Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  journal={arXiv preprint arXiv:1506.06579},
  year={2015}
}
@incollection{Samek2017,
author = {Samek, Wojciech and Binder, Alexander and Montavon, Gregoire and Lapuschkin, Sebastian and Muller, Klaus-Robert},
doi = {10.1109/TNNLS.2016.2599820},
file = {:Users/lukasmasopust/Documents/Informatik/4. Semester/Wissenschaftliches Arbeiten/Teil 2/papes/1509.06321.pdf:pdf},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
month = {nov},
number = {11},
pages = {2660--2673},
title = {{Evaluating the Visualization of What a Deep Neural Network Has Learned}},
url = {http://ieeexplore.ieee.org/document/7552539/},
volume = {28},
year = {2017}
}
@article{Nguyen2016,
abstract = {We can better understand deep neural networks by identifying which features each of their neurons have learned to detect. To do so, researchers have created Deep Visualization techniques including activation maximization, which synthetically generates inputs (e.g. images) that maximally activate each neuron. A limitation of current techniques is that they assume each neuron detects only one type of feature, but we know that neurons can be multifaceted, in that they fire in response to many different types of features: for example, a grocery store class neuron must activate either for rows of produce or for a storefront. Previous activation maximization techniques constructed images without regard for the multiple different facets of a neuron, creating inappropriate mixes of colors, parts of objects, scales, orientations, etc. Here, we introduce an algorithm that explicitly uncovers the multiple facets of each neuron by producing a synthetic visualization of each of the types of images that activate a neuron. We also introduce regularization methods that produce state-of-the-art results in terms of the interpretability of images obtained by activation maximization. By separately synthesizing each type of image a neuron fires in response to, the visualizations have more appropriate colors and coherent global structure. Multifaceted feature visualization thus provides a clearer and more comprehensive description of the role of each neuron.},
archivePrefix = {arXiv},
arxivId = {1602.03616},
author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
eprint = {1602.03616},
file = {::},
month = {feb},
title = {{Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks}},
url = {http://arxiv.org/abs/1602.03616},
year = {2016}
}
@article{Li2017,
  author    = {Hao Li and
               Zheng Xu and
               Gavin Taylor and
               Tom Goldstein},
  title     = {Visualizing the Loss Landscape of Neural Nets},
  journal   = {CoRR},
  volume    = {abs/1712.09913},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.09913},
  archivePrefix = {arXiv},
  eprint    = {1712.09913},
  timestamp = {Mon, 13 Aug 2018 16:48:49 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1712-09913},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{He2015,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{Liu2018, 
author={M. {Liu} and J. {Shi} and K. {Cao} and J. {Zhu} and S. {Liu}}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Analyzing the Training Processes of Deep Generative Models}, 
year={2018}, 
volume={24}, 
number={1}, 
pages={77-87}, 
keywords={data analysis;data visualisation;learning (artificial intelligence);time series;machine learning;credit assignment algorithm;visual clutter reduction;blue-noise polyline sampling scheme;visual analytics approach;training DGM;training dynamics;deep generative models;training failure;failed training process;time series samples;Training;Neurons;Time series analysis;Tools;Visual analytics;Analytical models;deep learning;deep generative models;blue noise sampling;credit assignment}, 
doi={10.1109/TVCG.2017.2744938}, 
ISSN={1077-2626}, 
month={Jan},}
@article{Hohman2018,
  title={Visual analytics in deep learning: An interrogative survey for the next frontiers},
  author={Hohman, Fred Matthew and Kahng, Minsuk and Pienta, Robert and Chau, Duen Horng},
  journal={IEEE transactions on visualization and computer graphics},
  year={2018},
  publisher={IEEE}
}
@article{Liu2016,
  title={Towards better analysis of deep convolutional neural networks},
  author={Liu, Mengchen and Shi, Jiaxin and Li, Zhen and Li, Chongxuan and Zhu, Jun and Liu, Shixia},
  journal={IEEE transactions on visualization and computer graphics},
  volume={23},
  number={1},
  pages={91--100},
  year={2016},
  publisher={IEEE}
}